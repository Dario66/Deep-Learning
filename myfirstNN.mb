Now is your turn. Modify:  

1- the definition of the hidden function  
2- the number of layers/neurons per layer; you just have to change inner_layers_dims in block 6.  


**Esercizio 1**  
Per esempio sostituisco la sigmoide con la Relu creando una nuova funzione:  

> def myhiddenfunction(x):  
> #define your favourite function    
> #output in range 0-1 if last activation is a sigmoid!  
> return (np.sin(x)**2 + np.cos(x)/3 + 1)/3  

> def relu(x):    
>   return np.maximum(0,x)  
  
> def generator(batchsize):  
  >  while True:  
   >   inputs = np.random.uniform(low=-np.pi,high=np.pi,size=batchsize)  
    >  outputs = np.zeros(batchsize)  
     > for i in range(0,batchsize):  
      >    outputs[i] = myhiddenfunction(inputs[i])  
      > yield (inputs,outputs)  
